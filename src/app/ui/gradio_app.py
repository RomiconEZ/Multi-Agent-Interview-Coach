"""
Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Multi-Agent Interview Coach.
"""

from __future__ import annotations

import asyncio
import logging

from pathlib import Path
from typing import Any

import gradio as gr

from ..core.config import settings
from ..core.logger_setup import get_system_logger, setup_logging
from ..interview import InterviewSession, create_interview_session

logger: logging.LoggerAdapter[logging.Logger] = get_system_logger(__name__)

_current_session: InterviewSession | None = None
_last_log_path: Path | None = None
_last_detailed_log_path: Path | None = None


def _run_async(coro: Any) -> Any:
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    if loop.is_running():
        import nest_asyncio

        nest_asyncio.apply()

    return loop.run_until_complete(coro)


async def _start_interview_async(
    model: str,
) -> tuple[str, str, list[tuple[str | None, str | None]]]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é.

    :return: Tuple (—Å—Ç–∞—Ç—É—Å, –æ—á–∏—â–µ–Ω–Ω—ã–π –∏–Ω–ø—É—Ç, –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞).
    """
    global _current_session, _last_log_path, _last_detailed_log_path

    if _current_session is not None:
        await _current_session.close()

    _last_log_path = None
    _last_detailed_log_path = None

    model_name = model.strip() if model.strip() else None
    _current_session = await create_interview_session(model_name)

    greeting = await _current_session.start()

    status = f"‚úÖ –ò–Ω—Ç–µ—Ä–≤—å—é –Ω–∞—á–∞—Ç–æ | –ú–æ–¥–µ–ª—å: {_current_session._llm_client.model}"
    history: list[tuple[str | None, str | None]] = [(None, greeting)]

    return status, "", history


def start_interview(model: str) -> tuple[str, str, list[tuple[str | None, str | None]]]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ –∏–Ω—Ç–µ—Ä–≤—å—é."""
    return _run_async(_start_interview_async(model))


async def _send_message_async(
    message: str,
    history: list[tuple[str | None, str | None]],
) -> tuple[str, str, list[tuple[str | None, str | None]], str, str | None, str | None]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ.

    :return: Tuple (—Å—Ç–∞—Ç—É—Å, –æ—á–∏—â–µ–Ω–Ω—ã–π –∏–Ω–ø—É—Ç, –∏—Å—Ç–æ—Ä–∏—è, —Ñ–∏–¥–±—ç–∫, –ø—É—Ç—å_–ª–æ–≥, –ø—É—Ç—å_–¥–µ—Ç–∞–ª—å–Ω—ã–π).
    """
    global _current_session, _last_log_path, _last_detailed_log_path

    if _current_session is None:
        return "‚ùå –°–Ω–∞—á–∞–ª–∞ –Ω–∞—á–Ω–∏—Ç–µ –∏–Ω—Ç–µ—Ä–≤—å—é", message, history, "", None, None

    if not message.strip():
        return "‚ùå –í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", "", history, "", None, None

    history.append((message, None))

    response, is_finished = await _current_session.process_message(message.strip())

    history[-1] = (message, response)

    if is_finished:
        feedback, summary_path, detailed_path = await _current_session.generate_feedback()
        feedback_text = feedback.to_formatted_string()

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
        metrics = _current_session.get_session_metrics()
        if metrics:
            feedback_text += "\n\n" + metrics.to_summary_string()

        _last_log_path = summary_path
        _last_detailed_log_path = detailed_path

        status = "‚úÖ –ò–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–¥–±—ç–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω."
        return status, "", history, feedback_text, str(summary_path), str(detailed_path)

    status = (
        f"‚úÖ –•–æ–¥ {_current_session.state.current_turn if _current_session.state else '?'}"
    )
    return status, "", history, "", None, None


def send_message(
    message: str,
    history: list[tuple[str | None, str | None]],
) -> tuple[str, str, list[tuple[str | None, str | None]], str, str | None, str | None]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è."""
    return _run_async(_send_message_async(message, history))


async def _stop_interview_async(
    history: list[tuple[str | None, str | None]],
) -> tuple[str, list[tuple[str | None, str | None]], str, str | None, str | None]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é."""
    global _current_session, _last_log_path, _last_detailed_log_path

    if _current_session is None:
        return "‚ùå –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é", history, "", None, None

    if _current_session._state:
        _current_session._state.is_active = False

    feedback, summary_path, detailed_path = await _current_session.generate_feedback()
    feedback_text = feedback.to_formatted_string()

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
    metrics = _current_session.get_session_metrics()
    if metrics:
        feedback_text += "\n\n" + metrics.to_summary_string()

    _last_log_path = summary_path
    _last_detailed_log_path = detailed_path

    history.append(("–°—Ç–æ–ø –∏–Ω—Ç–µ—Ä–≤—å—é", "–ò–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–æ—Ä–º–∏—Ä—É—é —Ñ–∏–¥–±—ç–∫..."))

    return (
        "‚úÖ –ò–Ω—Ç–µ—Ä–≤—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–æ",
        history,
        feedback_text,
        str(summary_path),
        str(detailed_path),
    )


def stop_interview(
    history: list[tuple[str | None, str | None]],
) -> tuple[str, list[tuple[str | None, str | None]], str, str | None, str | None]:
    return _run_async(_stop_interview_async(history))


def create_gradio_interface() -> gr.Blocks:
    with gr.Blocks(title="Multi-Agent Interview Coach", theme=gr.themes.Soft()) as app:
        gr.Markdown(
            """
            # üéØ Multi-Agent Interview Coach

            –°–∏—Å—Ç–µ–º–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é —Å AI-–∞–≥–µ–Ω—Ç–∞–º–∏:
            - **Observer Agent** ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã, –≤—ã—è–≤–ª—è–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
            - **Interviewer Agent** ‚Äî –≤–µ–¥—ë—Ç –¥–∏–∞–ª–æ–≥, –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            - **Evaluator Agent** ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–¥–±—ç–∫

            **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
            1. –ù–∞–∂–º–∏—Ç–µ "üöÄ –ù–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é"
            2. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ—Å—å (–∏–º—è, –ø–æ–∑–∏—Ü–∏—è, –æ–ø—ã—Ç)
            3. –û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
            4. –°–∫–∞–∂–∏—Ç–µ "—Å—Ç–æ–ø" –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∏–¥–±—ç–∫–∞
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

                model_input = gr.Textbox(
                    label="–ú–æ–¥–µ–ª—å LLM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                    placeholder=settings.LITELLM_MODEL,
                    value="",
                )

                start_btn = gr.Button("üöÄ –ù–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤—å—é", variant="primary")
                stop_btn = gr.Button("üõë –ó–∞–≤–µ—Ä—à–∏—Ç—å –∏ –ø–æ–ª—É—á–∏—Ç—å —Ñ–∏–¥–±—ç–∫", variant="stop")

                status_output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å", interactive=False)

            with gr.Column(scale=2):
                gr.Markdown("### üí¨ –î–∏–∞–ª–æ–≥")

                chatbot = gr.Chatbot(label="–ò–Ω—Ç–µ—Ä–≤—å—é", height=400, type="tuples")

                with gr.Row():
                    msg_input = gr.Textbox(
                        label="–í–∞—à –æ—Ç–≤–µ—Ç",
                        placeholder="–í–≤–µ–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç...",
                        lines=2,
                        scale=4,
                    )
                    send_btn = gr.Button("üì§ –û—Ç–ø—Ä–∞–≤–∏—Ç—å", scale=1)

        with gr.Row():
            with gr.Column():
                gr.Markdown("### üìä –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–¥–±—ç–∫")
                feedback_output = gr.Textbox(label="–§–∏–¥–±—ç–∫", lines=20, interactive=False)

            with gr.Column():
                gr.Markdown("### üìÅ –°–∫–∞—á–∞—Ç—å –ª–æ–≥–∏")

                main_log_file = gr.File(label="üìÑ –û—Å–Ω–æ–≤–Ω–æ–π –ª–æ–≥", interactive=False)
                detailed_log_file = gr.File(label="üìÑ –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥", interactive=False)

        start_btn.click(
            fn=start_interview,
            inputs=[model_input],
            outputs=[status_output, msg_input, chatbot],
        )

        send_btn.click(
            fn=send_message,
            inputs=[msg_input, chatbot],
            outputs=[
                status_output,
                msg_input,
                chatbot,
                feedback_output,
                main_log_file,
                detailed_log_file,
            ],
        )

        msg_input.submit(
            fn=send_message,
            inputs=[msg_input, chatbot],
            outputs=[
                status_output,
                msg_input,
                chatbot,
                feedback_output,
                main_log_file,
                detailed_log_file,
            ],
        )

        stop_btn.click(
            fn=stop_interview,
            inputs=[chatbot],
            outputs=[
                status_output,
                chatbot,
                feedback_output,
                main_log_file,
                detailed_log_file,
            ],
        )

    return app


def launch_app(
    server_name: str = "0.0.0.0",
    server_port: int = 7860,
    share: bool = False,
) -> None:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Gradio –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ.

    :param server_name: –•–æ—Å—Ç —Å–µ—Ä–≤–µ—Ä–∞.
    :param server_port: –ü–æ—Ä—Ç —Å–µ—Ä–≤–µ—Ä–∞.
    :param share: –°–æ–∑–¥–∞—Ç—å –ø—É–±–ª–∏—á–Ω—É—é —Å—Å—ã–ª–∫—É.
    """
    setup_logging()
    logger.info(f"Launching Gradio app on {server_name}:{server_port}")

    app = create_gradio_interface()
    app.launch(server_name=server_name, server_port=server_port, share=share)
